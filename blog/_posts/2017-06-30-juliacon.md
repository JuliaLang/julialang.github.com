---
layout: post
title:  JuliaCon on the West Coast
author: Ranjan Anantharaman
---

It’s a year later and I’m back: this time to the West Coast for the next edition of JuliaCon. 
The first day was workshop day. The first [workshop](https://www.youtube.com/watch?v=7NDkpWoNiQ4) was on the [DifferentialEquations](https://github.com/JuliaDiffEq/) ecosystem, 
championed by Chris Rackauckas, who spoke about his vision to make the ecosystem the 
scientist’s one stop shop for simulations. Next workshop: [`Optim.jl`](https://github.com/JuliaNLSolvers/Optim.jl). Statistical learning
 problems are often minimization problems where the objective function to minimize is your
error function. The Optim ecosystem provides a rich variety of techniques to solve the
equation, with a rich selection of options and callbacks for the user to understand why
his problem hasn’t converged. The last workshop was on [machine learning](https://github.com/ninjin/juliacon2017_dl_workshop) in Julia. It started
explaining concepts in machine learning from the ground up and then coding them up in pure Julia. Then we went into neural networks and deep learning before writing a recurrent network in pure Julia. 

The day in retrospect turned out to be quite heavy, but I had to ran back home to start prepping for my talks. 
   
The first day’s [keynote](https://www.youtube.com/watch?v=DUdE3M2nlDE) presented by Fernando Perez, a research scientist at LBNL, was on 
using online software and tools such as Binder that can render Jupyter notebooks, and how 
this would help researchers share their plots and results in the form of a notebook. Stefan’s
[Pkg3](https://github.com/StefanKarpinski/Pkg3.jl) talk spoke about the problems with the old 
package manager and then spoke about how his work on Pkg3 would solve some of those problems. 
At this point, I decided to go brush up for my talk on [Miletus](https://www.youtube.com/watch?v=FKBSVb9405w), a Julia package to model financial contracts 
and then to value them using a suite of algorithms. In the afternoon, my colleague Jameson 
attempted to break down the inner workings of the Julia compiler and some future directions he might want to take it, after which Tim Besard spoke about his work on native GPU code-generation, where he intercepts LLVM IR to generate PTX code that runs directly on the GPU. 
    
[Flux](https://github.com/MikeInnes/Flux.jl) is a new Julia package for machine learning that 
aims to treats models as functions with tunable parameters. Through use of macros and 
functional programming paradigms, Flux’s ease of use would allow developers to design complex 
neural network architectures, and step through each layer like anyone would a normal Julia 
function. While Flux is meant to act as a specification library, a package like 
[`KNet.jl`](https://github.com/denizyuret/Knet.jl) aims to act like a proper computational 
backend. It uses dynamic computational graphs and uses 
[automatic differentiation](https://github.com/denizyuret/AutoGrad.jl) to get the 
gradients of any Julia function. The package uses several high level language features which 
the author claims is missing from other frameworks that work with static computational 
graphs. 
     
The second day started with Mykel Kochenderfer’s work with the FAA and Lincoln Labs on 
collision avoidance systems. He went into detail about the complex decision processes that he 
has to work with under uncertainty. This followed by Jeff’s review of the type system 
overhaul. This, for me, was one of the best presentations at the conference, for how Jeff 
broke down fairly complex ideas into a simple ones for general consumption. I also got a 
chance to listen for the first time to a talk on probabilistic programming, and I’m still not 
a hundred percent sure I know what it is ( :-) ). The next talk spoke about multidimensional 
signal processing, especially focussing on the Shear Transform. I was delighted to know that 
the author uses ArrayFire.jl for his research, and that it helps him greatly. 
      
The Celeste keynote though was the highlight of the day, with my colleague Keno getting into 
the details of the computation. He spoke about how he had to make several improvements to the 
Julia compiler so as to make memory access patterns more uniform and hence more efficient, 
and how they even used Julia packages almost as is for the project. Not only was it 
remarkable that they crossed 1 Petaflop (which is probably the first for a 

high productivity language), they managed to do so without writing hand tuned Ninja code. 
This, for me, was the biggest achievement. At this point I had to run off to prepare for my 
second talk at the conference: Circuitscape, a landscape modelling tool that was used to 
calculate least resistance paths across large swathes of land. I went into detail about my 
vision for the package and how I will take it there, and hopefully earned a couple of 
collaborators. 
 
At the end of the day, there was a poster session with GSoC students presenting their work 
over posters. They were all great. It was in particular nice to see an old intern of ours, 
Divyansh, do some great work on parallel graph algorithms. 
        
The last day of the conference started Kathy Yelick from UC Berkeley talking about how the 
HPC world and Big Data world as sort of merging, and how researchers need to find ways to 
tackle larger amounts of scientific data. Then it was time for a lighter session with Jiahao 
talking about “how to take vector transposes seriously”. He went into 
the history of the famous issue #4774, much of which involved heated discussion on the 
different notions of the word “vector”. The afternoon started out with Stefan speaking 
briefly about the roadmap for Julia 1.0, assuring the community that 1.0 was still on despite 
the latest master being called 0.7-DEV. This was followed up by a very interesting session 
on why Julia is a great language for mathematical programming by Madeleine Udell, and focused 
on the package Convex.jl, which infers and formulates convex optimization problems. Convex 
and JuMP are the two jewels on the crest of a robust optimization ecosystem in Julia. 
         
The conference ended with Jeff Bezanson presenting a short talk on JuliaDB, the in-memory 
database that was part of the JuliaFin product from Julia Computing, and was recently open 
sourced. It supports both relational and SQL-like queries and uses Dagger.jl under the hood. 
After Jeff finished, the question and answer session seemed to stretch for longer than the 
talk itself, indicating an unwillingness amongst the participants for the conference to end. 
          
While it was certainly a hectic few days with meetings upon meetings with collaborators and 
plenty of information, I found that I could look back on the week with a sense of 
satisfaction that only a JuliaCon can bring. 
           

