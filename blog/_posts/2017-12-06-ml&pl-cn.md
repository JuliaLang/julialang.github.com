---
layout: post
title:  机器学习与编程语言
---

> 任何足够复杂的机器学习系统，里面都拼凑了半个不规范，处处错误，且运行缓慢的编程语言[^greenspun]

[^greenspun]: 引申自 [Philip Greenspun](https://en.wikipedia.org/wiki/Greenspun%27s_tenth_rule)

<div style="font-size:75%">
作者： Mike Innes (Julia Computing), David Barber (UCL), Tim Besard (UGent), James Bradbury (Salesforce Research), Valentin Churavy (MIT), Simon Danisch (MIT), Alan Edelman (MIT), Stefan Karpinski (Julia Computing), Jon Malmaud (MIT), Jarrett Revels (MIT), Viral Shah (Julia Computing), Pontus Stenetorp (UCL) 和 Deniz Yuret (Koç University) 译者：李治中（香港中文大学）
</div>

<span class="drop">机</span>器学习（ML）如今已是爆炸般的火热。作为编程语言（PL）领域的工作者，我们饶有兴趣地观注机器学习中越来越复杂的模型，以及构造这些模型所用的框架。当今最前沿的模型越来越像*程序*：比如有些模型就支持循环或递归这样的编程结构；而这也对创作机器学习模型的工具本身，即编程语言，提出许多有趣的挑战。

虽说目前机器学习还没有一个专属的语言，但事实上已有几种新语言藏身于 Python 语言接口之后（比如 TensorFlow）;而另一些新语言（像 PyTorch）则直接用 Python 作为自己的建模语言。我们要问——到底需不需要一个专门为机器学习定制的全新的语言？如果是，为什么？更重要的问题，如果将来有一个理想的机器学习语言，它会长什么样？

## [儿童黑话](https://zh.wikipedia.org/zh-hk/兒童黑話)及其它隐匿语言

TensorFlow（TF）和它的同类[^tf][已被公认属于编程语言](https://dl.acm.org/citation.cfm?doid=3088525.3088527)，尽管它们在某些方面还有局限。这看起来有些意外，毕竟大家 TF 编程用的是 Python。然而琢磨一下会发现，TF 只是让你用 Python 代码[构造了一个表达式树](https://www.tensorflow.org/programmers_guide/graphs)，这个表达式树是用内部语言表示的，然后 TF 再对它进行计算。

[^tf]: 本文以 TensorFlow 为例。读者可以自行替换为其它“先定义—后计算”类型的框架，比如 CNTK 或 MXNet。

实际上，你可以在任何语言上实现 TensorFlow 这种“延迟”计算的风格。
如下的 JavaScript 代码就用此风格写了一个常见的函数（`add`）:

{% highlight javascript %}
function add(a,b) {
  return `${a}+${b}`;
}
x = 1; y = 2
z = add('x', 'y') // 'x+y'
eval(z) // 3
x = 4
eval(z) // 6
{% endhighlight %}

这就是*元编程*——写写代码的代码。这个例子里的元语言和目标语言是一致的（都是 JavaScript），但它们完全可以是两种不同的语言（如 C 语言预处理器之于 C 语言）。同理，元编程也可以通过某种数据结构（[AST](https://en.wikipedia.org/wiki/Abstract_syntax_tree)）来实现，而不仅限于字符串层面的处理。对 TensorFlow 而言，Python 就是一个用来书写 TF 基于图的编程语言[^ast]的元语言。如果您觉得说服力不够，那么请想一想，TensorFlow 在支持图的[变量作用域](https://www.tensorflow.org/programmers_guide/variables)和[流程控制](https://www.tensorflow.org/api_docs/python/tf/cond)这些编程结构时甚至都没有采用 Python 的语法，而是通过 API 来实现的。

[^ast]: TensorFlow 的图本质上是一种基于数据流的抽象语法树（AST）。

TensorFlow 和类似的工具以“我仅仅是个库”的姿态出现。然而作为库，它们却异乎寻常。大多数的库只提供简单的一组函数和数据结构，而不会给一整套的新语言和运行时。那么，采用这样一种复杂的途径真的有必要吗？

## 为什么要创造一个新语言？

根本的原因其实非常简单：机器学习研究已经对计算有着很高的要求——如果能简化建模语言，那么我们就可以更容易地加入针对于特定领域的优化与功能。训练模型需要语言对硬件有极好的支持，拥有良好的数值库，很低的解释器开销，并且要支持多种并行方式。像 Python 这样的通用语言很难满足这些要求，而 TensorFlow 却能轻松胜任。

话虽如此，有一个小问题也不能忽略。繁华有效的优化依赖于简化的假定（机器学习模型里不会出现递归，也不需要自定义梯度，是吧？）。正是这些假定使得优化更容易，也使得算法更易于部署到小型设备上。可惜，研究员们沉浸于破坏这些假定的乐趣中，工程师们面对的模型越来越复杂。现在，一个模型可能要求条件分支（没事，容易搞定），递归循环（不怎么容易但有可能搞定），甚至是[树的递归](https://arxiv.org/pdf/1503.00075.pdf)（基本上做不到）。 在机器学习的很多分支中，比如[神经网络](https://blog.keras.io/the-future-of-deep-learning.html)和[概率编程](https://eng.uber.com/pyro/)，模型越来越像程序。其中有些模型可以推导*其它*程序（例如[程序生成器](https://arxiv.org/pdf/1705.03633.pdf)和[解释器](https://arxiv.org/abs/1605.06640)）；有些则包括像蒙特卡罗树搜索这样的不可导组件。在保证最高性能的同时提供充分的灵活性，这对运行时的构建提出了极高的挑战；然而两者的兼顾却是最强大的模型和突破性的成果越来越倚重的。

<img src="/images/sentiment-treebank.png"/>
<div class="desc">
  用机器学习处理复杂的树结构数据，如<a href="https://nlp.stanford.edu/sentiment/treebank.html">斯坦福情绪树库</a>，用到了可微的，递归的算法。
</div>

这种方式，至少以当前的外在形式，还有一个客观的不利因素，就是需要引入我们之前讨论过的元编程。构建和计算表达式树给程序员和编译器都额外施加了极大的负担。代码会有两个运行时间，而且每个都在不同的语言语义下，像单步调试这样的操作就极其困难，这些因素都使得程序变得难以理解。我们可以通过为这个新运行时创造一个句法语言来解决问题，但这也意味着几乎创造了一个完整的编程语言。在我们已经有流行的数值语言的情况下，这么做还有意义吗？

## 直接用 Python 不行吗？

当机器学习模型开始需要一个编程语言的全部能力时，Chainer 等探索了一种
[“运行即定义”](https://arxiv.org/pdf/1701.03980.pdf)的方式，Python 程序本身在这种方式里被看作模型，梯度则由运行时自动微分（AD）来取得。从可用性角度来看这非常炫酷：如果你想要一个处理树结构的递归模型，只要把这个过程写下来，然后看着 AD 变魔术就行啦！这种方式带来的感觉上的变化[怎么夸都不过分](https://twitter.com/karpathy/status/868178954032513024?lang=en)，并且一种可以毫无阻碍就可以试验新想法的方式对学术研究也是无价之宝。

然而，让 Python 适应机器学习所要求的巨大计算量要比想象中的困难的多。
人们投入了[大量的工作](https://www.youtube.com/watch?v=DBVLcgq2Eg0)去复现那些在快速语言看来是轻而易举的优化。许多让 Python 更快些的努力已长眠于编程语言的埋骨地，有些[很知名](https://arstechnica.com/information-technology/2009/03/google-launches-project-to-boost-python-performance-by-5x/)，但它们都[失败](https://blog.pyston.org/2017/01/31/pyston-0-6-1-released-and-future-plans/)了。 [Python 的语义](http://blog.kevmod.com/2017/02/personal-thoughts-about-pystons-outcome/)也从根本上让模型级别的并行化和编译模型到小型设备这两件事变得很困难。

为 MXNet 而设计的 [Gluon](https://mxnet.incubator.apache.org/api/python/gluon.html) 正在寻求一种鱼与熊掌兼得的方式，至少某种程度上想达到这个目标。它的想法是把基本的动态 AD 与代码跟踪结合起来得到“静态子图”，进而进行优化。可惜这样做的结果就是把一些迥然不同的实现与 API 混搅在了一起。这种做法也有局限；图在 MXNet 中不仅被用于核心级别的优化，也被用于高级的图调度，比如[把一个模型分到多个 GPU 上](https://mxnet.incubator.apache.org/how_to/multi_devices.html)。我们不清楚除了为那些支持动态计算的图容器额外加入新 API 之外，这些混合方案还有什么办法可以做到这件事。

## What might a tailor-made ML language look like?

There are few domains as demanding about language-level design issues as machine learning. But it’s not unprecedented, and in areas like [formal reasoning and verification](https://coq.inria.fr/) or [cluster computing](https://chapel-lang.org/), new, tailor-made languages have proved an effective solution. Similarly, we expect to see new or existing languages customised for the kind of numerical, differentiable, parallelisable and even probabilistic computations needed in ML.

An obvious current challenge for ML languages is achieving generality alongside performance, and the early hybrid approaches will need much more development. We expect that future ML runtimes will need to support arbitrary mixing of approaches (computational graphs that are static within dynamic within static …) and will need to get better at compiling dynamic code for deployment. Ideally, there will only be single, flexible "graph format" (or AST). The AST should have a syntax and statically describe dynamic behaviour (e.g. with a written `for` loop) – in other words, it should look a lot more like a standard programming language.

*Programmable semantics* would open up new levels of flexibility, and could be provided by a feature similar to macros. This would allow features like multi-GPU training to be built on top of the core system, by specifying where the code should have pure dataflow semantics (as opposed to standard imperative semantics, which are more flexible but may include side-effects that are unsafe to optimise). It could also allow the kinds of program manipulation needed by probabilistic programming languages, or the [vectorisation](https://www.cs.cmu.edu/~guyb/papers/Ble90.pdf) (batching) passes usually implemented by hand in NLP models.

As well as the PL community, ML engineers should pay close attention to the traditional Automatic Differentiation (AD) community. ML languages can take inspiration from pioneering work on [languages designed for truly first-class derivative](https://arxiv.org/pdf/1611.03416.pdf) support. Such languages can easily mix symbolic with runtime techniques (helping with the tradeoffs mentioned above), mix forward and reverse mode AD (for improved performance and memory usage), and [differentiate GPU kernels](http://mikeinnes.github.io/2017/08/24/cudanative.html) – all with no loss in performance.

ML research will increasingly need more powerful type systems, user-defined types and more means for extension. Gone are the days when it was enough to hard-code support for strided arrays on NVIDIA GPUs; cutting-edge techniques like [sparse machine learning](https://people.eecs.berkeley.edu/~elghaoui/Pubs/cidu2011_final.pdf), new hardware like [TPUs](https://cloud.google.com/tpu/), [Nervana](https://www.intelnervana.com/) and [FPGAs](https://www.forbes.com/sites/moorinsights/2017/08/28/microsoft-fpga-wins-versus-google-tpus-for-ai/#118733643904), and diverse deployment targets like [ARM chips](http://www.wired.co.uk/article/google-raspberry-pi-ai) or the [iPhone’s CoreML](https://developer.apple.com/documentation/coreml) chip all call for greater levels of flexibility. [Large-scale refactoring of core C++ code](https://github.com/tensorflow/tensorflow/pull/5267/files) for each new development will not scale.

Consider a world where adding new hardware support – or new kinds of data representations – could easily be accomplished by a user in high-level code, without changes to the original system. Here we expect ML systems to take inspiration from existing numerical computing languages, which can [already handle these tasks](https://arxiv.org/pdf/1604.03410.pdf) with ease.

Type systems can also have safety benefits, but current ones are not suited to array-heavy code where array dimensions are meaningful (for example, spatial vs channel vs batch dimensions in images). These distinctions are left to [pure convention](https://github.com/pytorch/pytorch/issues/1220), and hairy dimension-permuting code has no protection from mistakes, leaving much room for more array-aware type systems. We expect the trend towards dynamic typing to continue,[^types] mainly due to practitioners’ preference for interactivity and scripting, but hope to see further innovations like [CNTK’s optionally dynamic dimensions](https://cntk.ai/pythondocs/sequence.html).

[^types]: Though we note that, internally, current systems span the gamut from fully dynamic (PyTorch and its ATen backend) to unusually static (TensorFlow’s XLA and MXNet, where all dimensions are known before the graph is run).

ML engineers are increasingly interested in traditional [software engineering problems](https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf) like the maintenance and extension of production systems. The [ML programming model](https://medium.com/@karpathy/software-2-0-a64152b37c35) makes it harder to create abstraction barriers and interfaces between components, and re-training of a model can easily break backwards compatibility. ML languages will likely be able to incorporate solutions to these problems just as regular languages do, but this remains an open design problem.

<div style="text-align:center">
<a href="https://xkcd.com/1838/">
<img height="350px" src="https://imgs.xkcd.com/comics/machine_learning_2x.png"/>
</a>
</div>
<div class="desc">
  Software Engineering 2.0? <i>(via XKCD)</i>
</div>

A downside to any new language is that it require a new library ecosystem, as only code written for the new runtime benefits from it. For example, rather than reusing the Python ecosystem, the TensorFlow developers need to rewrite libraries for tasks like [image processing](https://www.tensorflow.org/api_guides/python/image) and [file IO](https://www.tensorflow.org/api_docs/python/tf/TextLineReader) in the graph language, throwing out the vast effort behind projects like SciPy. This may well be the only way forward, but ML practitioners should not split from the wider numerical and HPC community. An ideal ML ecosystem is an ideal numerical one, and vice versa, and collaboration between these communities will multiply everyone’s efforts.

We expect to see these developments coming from several angles. Graph IRs and formats like [XLA](https://www.tensorflow.org/performance/xla/), [ONNX](https://github.com/onnx/onnx) and [NNVM](https://github.com/dmlc/nnvm) are becoming ever more sophisticated and will likely take more inspiration from traditional language design,[^chris] perhaps even adding surface syntax to become fully-fledged programming languages. TensorFlow’s XLA has started a push towards special-purpose compiler stacks that now includes [TVM](http://tvmlang.org/), [DLVM](http://dlvm.org/), [myelin](https://github.com/google/sling/tree/master/myelin), and other ongoing work. Meanwhile, projects like the [PyTorch JIT](https://github.com/pytorch/pytorch/tree/master/torch/csrc/jit), [Gluon](https://mxnet.incubator.apache.org/api/python/gluon.html) and [Tangent](https://github.com/google/tangent) are efforts to make Python itself a better modelling language, in spite of the significant challenges. Having just argued that ML is a numerical programming languages problem, we in the Julia community feel that it is an excellent substrate for experimenting with these kinds of language-level issues, and will continue to push the boundaries with projects like [Knet](https://github.com/denizyuret/Knet.jl), [Flux](https://fluxml.github.io/), [Cassette](https://github.com/jrevels/Cassette.jl), [CUDAnative](https://github.com/JuliaGPU/CUDAnative.jl), [DataFlow.jl](https://github.com/MikeInnes/DataFlow.jl), and more.

[^chris]: Google Brain’s increasing hiring of programming languages experts, such as [Chris Lattner](https://techcrunch.com/2017/08/14/swift-creator-chris-lattner-joins-google-brain-after-tesla-autopilot-stint/), is an interesting development on this point.

## Conclusion: An Inference about Machine Learning

Machine learning models have become extremely general information-processing systems that build ever higher-level and more complex abstractions; recurrence, recursion, higher-order models, even [stack machines](https://nlp.stanford.edu/blog/hybrid-tree-sequence-neural-networks-with-spinn/) and [language interpreters](https://arxiv.org/abs/1605.06640), all implemented as compositions of basic components. ML is a new programming paradigm, albeit a strange one that’s heavily numerical, differentiable and parallel. And as in any engineering field, the tooling available will have a profound impact on the scope and quality of future work.

All this suggests that designers of ML systems have a momentous challenge ahead of them. But while that’s true, there’s some good news: The very same problems have been deeply explored, if not already solved, by language researchers over the last few decades! To really take this new field to its full potential, the machine learning and programming languages communities will have to combine forces, and the real challenge is to integrating the disparate expertise of these two groups.

Can we build systems that treat numerics, derivatives and parallelism as first-class features, without sacrificing traditional programming ideas and wisdom? This is the foundational question which languages over the coming decade will have to answer.
