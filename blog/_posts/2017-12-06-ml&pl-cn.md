---
layout: post
title:  机器学习与编程语言
---

> 任何足够复杂的机器学习系统，里面都拼凑了半个不规范，处处错误，且运行缓慢的编程语言[^greenspun]

[^greenspun]: 引申自 [Philip Greenspun](https://en.wikipedia.org/wiki/Greenspun%27s_tenth_rule)

<div style="font-size:75%">
作者： Mike Innes (Julia Computing), David Barber (UCL), Tim Besard (UGent), James Bradbury (Salesforce Research), Valentin Churavy (MIT), Simon Danisch (MIT), Alan Edelman (MIT), Stefan Karpinski (Julia Computing), Jon Malmaud (MIT), Jarrett Revels (MIT), Viral Shah (Julia Computing), Pontus Stenetorp (UCL) 和 Deniz Yuret (Koç University) 译者：李治中（香港中文大学）
</div>

<span class="drop">机</span>器学习（ML）如今已是爆炸般的火热。作为编程语言（PL）领域的工作者，我们饶有兴趣地观注机器学习中越来越复杂的模型，以及构造这些模型所用的框架。当今最前沿的模型越来越像*程序*：比如有些模型就支持循环或递归这样的编程结构；而这也对创作机器学习模型的工具本身，即编程语言，提出许多有趣的挑战。

虽说目前机器学习还没有一个专属的语言，但事实上已有几种新语言藏身于 Python 语言接口之后（比如 TensorFlow）;而另一些新语言（像 PyTorch）则直接用 Python 作为自己的建模语言。我们要问——到底需不需要一个专门为机器学习定制的全新的语言？如果是，为什么？更重要的问题，如果将来有一个理想的机器学习语言，它会长什么样？

## [儿童黑话](https://zh.wikipedia.org/zh-hk/兒童黑話)及其它隐匿语言

TensorFlow（TF）和它的同类[^tf][已被公认属于编程语言](https://dl.acm.org/citation.cfm?doid=3088525.3088527)，尽管它们在某些方面还有局限。这看起来有些意外，毕竟大家 TF 编程用的是 Python。然而琢磨一下会发现，TF 只是让你用 Python 代码[构造了一个表达式树](https://www.tensorflow.org/programmers_guide/graphs)，这个表达式树是用内部语言表示的，然后 TF 再对它进行计算。

[^tf]: 本文以 TensorFlow 为例。读者可以自行替换为其它“先定义—后计算”类型的框架，比如 CNTK 或 MXNet。

实际上，你可以在任何语言上实现 TensorFlow 这种“延迟”计算的风格。
如下的 JavaScript 代码就用此风格写了一个常见的函数（`add`）:

{% highlight javascript %}
function add(a,b) {
  return `${a}+${b}`;
}
x = 1; y = 2
z = add('x', 'y') // 'x+y'
eval(z) // 3
x = 4
eval(z) // 6
{% endhighlight %}

这就是*元编程*——写写代码的代码。这个例子里的元语言和目标语言是一致的（都是 JavaScript），但它们完全可以是两种不同的语言（如 C 语言预处理器之于 C 语言）。同理，元编程也可以通过某种数据结构（[AST](https://en.wikipedia.org/wiki/Abstract_syntax_tree)）来实现，而不仅限于字符串层面的处理。对 TensorFlow 而言，Python 就是一个用来书写 TF 基于图的编程语言[^ast]的元语言。如果您觉得说服力不够，那么请想一想，TensorFlow 在支持图的[变量作用域](https://www.tensorflow.org/programmers_guide/variables)和[流程控制](https://www.tensorflow.org/api_docs/python/tf/cond)这些编程结构时甚至都没有采用 Python 的语法，而是通过 API 来实现的。

[^ast]: TensorFlow 的图本质上是一种基于数据流的抽象语法树（AST）。

TensorFlow 和类似的工具以“我仅仅是个库”的姿态出现。然而作为库，它们却异乎寻常。大多数的库只提供简单的一组函数和数据结构，而不会给一整套的新语言和运行时。那么，采用这样一种复杂的途径真的有必要吗？

## 为什么要创造一个新语言？

根本的原因其实非常简单：机器学习研究已经对计算有着很高的要求——如果能简化建模语言，那么我们就可以更容易地加入针对于特定领域的优化与功能。训练模型需要语言对硬件有极好的支持，拥有良好的数值库，很低的解释器开销，并且要支持多种并行方式。像 Python 这样的通用语言很难满足这些要求，而 TensorFlow 却能轻松胜任。

话虽如此，有一个小问题也不能忽略。繁华有效的优化依赖于简化的假定（机器学习模型里不会出现递归，也不需要自定义梯度，是吧？）。正是这些假定使得优化更容易，也使得算法更易于部署到小型设备上。可惜，研究员们沉浸于破坏这些假定的乐趣中，工程师们面对的模型越来越复杂。现在，一个模型可能要求条件分支（没事，容易搞定），递归循环（不怎么容易但有可能搞定），甚至是[树的递归](https://arxiv.org/pdf/1503.00075.pdf)（基本上做不到）。 在机器学习的很多分支中，比如[神经网络](https://blog.keras.io/the-future-of-deep-learning.html)和[概率编程](https://eng.uber.com/pyro/)，模型越来越像程序。其中有些模型可以推导*其它*程序（例如[程序生成器](https://arxiv.org/pdf/1705.03633.pdf)和[解释器](https://arxiv.org/abs/1605.06640)）；有些则包括像蒙特卡罗树搜索这样的不可导组件。在保证最高性能的同时提供充分的灵活性，这对运行时的构建提出了极高的挑战；然而两者的兼顾却是最强大的模型和突破性的成果越来越倚重的。

<img src="/images/sentiment-treebank.png"/>
<div class="desc">
  用机器学习处理复杂的树结构数据，如<a href="https://nlp.stanford.edu/sentiment/treebank.html">斯坦福情绪树库</a>，用到了可微的，递归的算法。
</div>

这种方式，至少以当前的外在形式，还有一个客观的不利因素，就是需要引入我们之前讨论过的元编程。构建和计算表达式树给程序员和编译器都额外施加了极大的负担。代码会有两个运行时间，而且每个都在不同的语言语义下，像单步调试这样的操作就极其困难，这些因素都使得程序变得难以理解。我们可以通过为这个新运行时创造一个句法语言来解决问题，但这也意味着几乎创造了一个完整的编程语言。在我们已经有流行的数值语言的情况下，这么做还有意义吗？

## 直接用 Python 不行吗？

当机器学习模型开始需要一个编程语言的全部能力时，Chainer 等探索了一种
[“运行即定义”](https://arxiv.org/pdf/1701.03980.pdf)的方式，Python 程序本身在这种方式里被看作模型，梯度则由运行时自动微分（AD）来取得。从可用性角度来看这非常炫酷：如果你想要一个处理树结构的递归模型，只要把这个过程写下来，然后看着 AD 变魔术就行啦！这种方式带来的感觉上的变化[怎么夸都不过分](https://twitter.com/karpathy/status/868178954032513024?lang=en)，并且一种可以毫无阻碍就可以试验新想法的方式对学术研究也是无价之宝。

然而，让 Python 适应机器学习所要求的巨大计算量要比想象中的困难的多。
人们投入了[大量的工作](https://www.youtube.com/watch?v=DBVLcgq2Eg0)去复现那些在快速语言看来是轻而易举的优化。许多让 Python 更快些的努力已长眠于编程语言的埋骨地，有些[很知名](https://arstechnica.com/information-technology/2009/03/google-launches-project-to-boost-python-performance-by-5x/)，但它们都[失败](https://blog.pyston.org/2017/01/31/pyston-0-6-1-released-and-future-plans/)了。 [Python 的语义](http://blog.kevmod.com/2017/02/personal-thoughts-about-pystons-outcome/)也从根本上让模型级别的并行化和编译模型到小型设备这两件事变得很困难。

为 MXNet 而设计的 [Gluon](https://mxnet.incubator.apache.org/api/python/gluon.html) 正在寻求一种鱼与熊掌兼得的方式，至少某种程度上想达到这个目标。它的想法是把基本的动态 AD 与代码跟踪结合起来得到“静态子图”，进而进行优化。可惜这样做的结果就是把一些迥然不同的实现与 API 混搅在了一起。这种做法也有局限；图在 MXNet 中不仅被用于核心级别的优化，也被用于高级的图调度，比如[把一个模型分到多个 GPU 上](https://mxnet.incubator.apache.org/how_to/multi_devices.html)。我们不清楚除了为那些支持动态计算的图容器额外加入新 API 之外，这些混合方案还有什么办法可以做到这件事。

## 机器学习的专属语言可能长什么样？

极少有领域像机器学习这样对语言层面的设计有如此苛刻的要求。但并不是说没有先例，比如[形式推理与验证](https://coq.inria.fr/)和[集群计算](https://chapel-lang.org/)中就验证了，新的定制的语言是非常有效的解决方案。同样我们希望看到为机器学习所需的数值，微分，并行及概率计算等功能而定制的语言，不论是创造新的还是利用现有的。

当前对机器学习语言一个公认的挑战是达到兼顾通用性与性能，早期的混合方案在这方面还需多加努力。我们希望将来的机器学习进行时会支持任意的混合方式（静态的计算图包含在一个动态的图里，进而包含在另一个静态图里……），也更好的支持编译动态代码和部署。理想情况是只用一个灵活的“图格式”（或 AST）。这个 AST 有一种语法可以静态地描述动态行为（比如写一个 `for` 循环）——换言之，它更像一个标准的编程语言。

*可编程语义*开启了灵活性的新层次，而且它可以用类似于宏这样的功能来实现。这将的话，只要在核心系统上指定哪些代码是纯数据流语义就可以实现多 GPU 训练这样的功能（与此对照，标准的祈使语义更灵活，但能导致优化不安全的副作用）。它也可以实现概率编程语言所需要的操作，不然的话，像 NLP 模型中的[平行向量化处理](https://www.cs.cmu.edu/~guyb/papers/Ble90.pdf)（批处理）阶段就得手动去实现。

正如编程语言社区做的，机器学习工程师们也应密切关注传统的自动微分（AD）社区。机器学习语言可以从[真正内置支持微分的语言设计](https://arxiv.org/pdf/1611.03416.pdf)这些先驱性的工作中获得启迪。这种语言可以轻而易举地混合运用符号化与运行时技巧（便于权衡之前提到的因素），混合使用正向和反向自动微分（可以改进速度和内存占用），以及[对 GPU 核心微分](http://mikeinnes.github.io/2017/08/24/cudanative.html)——所有这些都没有以损失性能为代价。

机器学习研究中越来越依赖更强大的类型系统，需要用户自定义类型，也需要更多的扩展方式。硬编码就能支持 NVIDIA GPU 上的跨数组的美好时光已一去不返；像[稀疏机器学习](https://people.eecs.berkeley.edu/~elghaoui/Pubs/cidu2011_final.pdf)这样的前沿技术，[TPU](https://cloud.google.com/tpu/)，[Nervana](https://www.intelnervana.com/)，和 [FPGAs](https://www.forbes.com/sites/moorinsights/2017/08/28/microsoft-fpga-wins-versus-google-tpus-for-ai/#118733643904) 这样的全新硬件，[ARM 芯片](http://www.wired.co.uk/article/google-raspberry-pi-ai) 和 [iPhone 的 CoreML](https://developer.apple.com/documentation/coreml) 这样形形色色的部署目标，都要求灵活性要更上一层楼。每次新发展都[对核心 C++ 代码进行大规模重构](https://github.com/tensorflow/tensorflow/pull/5267/files)不是一个可持续的策略。

想象一个世界，增加一个新硬件的支持——或新的数据表示——能通过写高层代码来轻松解决，而不必对原有的系统做任何更改。我们期望机器学习系统从当前数值计算语言中汲取灵感，考虑到它们[已经能毫不费力地处理这些任务](https://arxiv.org/pdf/1604.03410.pdf)。

我们知道类型系统有安全性的优点，但当前的系统却不适用于现在充斥着数组的代码，因为数组的维度是有意义的（例如图像中有空间，通道和批次三种维度）。维度的区分[全靠约定](https://github.com/pytorch/pytorch/issues/1220)，繁杂的置换维度的代码的无法保证正确性，这表明完美地支持数组的类型系统还有很大的探索空间。鉴于实践者们对交互和脚本的偏好，我们预计动态类型的趋势仍将继续，[^types]但我们希望看到更多像 [CNTK 的可选动态维度](https://cntk.ai/pythondocs/sequence.html)这样的创新。

[^types]: 尽管我们知道，从彻头彻尾的动态（PyTorch 和它的 ATen 后端）到异乎寻常的静态（TensorFlow 的 XLA 和 MXNet 中，所有维度在图运算之前都已知），当前的系统内部设计已全范围覆盖。

机器学习工程师们对传统[软件工程问题](https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf)，如生产系统的维护与扩展，的兴趣日益增加。[机器学习编程模型](https://medium.com/@karpathy/software-2-0-a64152b37c35)模糊了组件间的抽象界限，让构造接口更困难，而且重新训练一个模型一不小心就破坏了身后兼容性。正如普通的语言，机器学习语言有希望解决这些问题，虽然怎么设计还是个公开问题。

<div style="text-align:center">
<a href="https://xkcd.com/1838/">
<img height="350px" src="https://imgs.xkcd.com/comics/machine_learning_2x.png"/>
</a>
</div>
<div class="desc">
  Software Engineering 2.0? <i>(via XKCD)</i>
</div>

A downside to any new language is that it require a new library ecosystem, as only code written for the new runtime benefits from it. For example, rather than reusing the Python ecosystem, the TensorFlow developers need to rewrite libraries for tasks like [image processing](https://www.tensorflow.org/api_guides/python/image) and [file IO](https://www.tensorflow.org/api_docs/python/tf/TextLineReader) in the graph language, throwing out the vast effort behind projects like SciPy. This may well be the only way forward, but ML practitioners should not split from the wider numerical and HPC community. An ideal ML ecosystem is an ideal numerical one, and vice versa, and collaboration between these communities will multiply everyone’s efforts.

We expect to see these developments coming from several angles. Graph IRs and formats like [XLA](https://www.tensorflow.org/performance/xla/), [ONNX](https://github.com/onnx/onnx) and [NNVM](https://github.com/dmlc/nnvm) are becoming ever more sophisticated and will likely take more inspiration from traditional language design,[^chris] perhaps even adding surface syntax to become fully-fledged programming languages. TensorFlow’s XLA has started a push towards special-purpose compiler stacks that now includes [TVM](http://tvmlang.org/), [DLVM](http://dlvm.org/), [myelin](https://github.com/google/sling/tree/master/myelin), and other ongoing work. Meanwhile, projects like the [PyTorch JIT](https://github.com/pytorch/pytorch/tree/master/torch/csrc/jit), [Gluon](https://mxnet.incubator.apache.org/api/python/gluon.html) and [Tangent](https://github.com/google/tangent) are efforts to make Python itself a better modelling language, in spite of the significant challenges. Having just argued that ML is a numerical programming languages problem, we in the Julia community feel that it is an excellent substrate for experimenting with these kinds of language-level issues, and will continue to push the boundaries with projects like [Knet](https://github.com/denizyuret/Knet.jl), [Flux](https://fluxml.github.io/), [Cassette](https://github.com/jrevels/Cassette.jl), [CUDAnative](https://github.com/JuliaGPU/CUDAnative.jl), [DataFlow.jl](https://github.com/MikeInnes/DataFlow.jl), and more.

[^chris]: Google Brain’s increasing hiring of programming languages experts, such as [Chris Lattner](https://techcrunch.com/2017/08/14/swift-creator-chris-lattner-joins-google-brain-after-tesla-autopilot-stint/), is an interesting development on this point.

## Conclusion: An Inference about Machine Learning

Machine learning models have become extremely general information-processing systems that build ever higher-level and more complex abstractions; recurrence, recursion, higher-order models, even [stack machines](https://nlp.stanford.edu/blog/hybrid-tree-sequence-neural-networks-with-spinn/) and [language interpreters](https://arxiv.org/abs/1605.06640), all implemented as compositions of basic components. ML is a new programming paradigm, albeit a strange one that’s heavily numerical, differentiable and parallel. And as in any engineering field, the tooling available will have a profound impact on the scope and quality of future work.

All this suggests that designers of ML systems have a momentous challenge ahead of them. But while that’s true, there’s some good news: The very same problems have been deeply explored, if not already solved, by language researchers over the last few decades! To really take this new field to its full potential, the machine learning and programming languages communities will have to combine forces, and the real challenge is to integrating the disparate expertise of these two groups.

Can we build systems that treat numerics, derivatives and parallelism as first-class features, without sacrificing traditional programming ideas and wisdom? This is the foundational question which languages over the coming decade will have to answer.
